Generator(
  (net): Sequential(
    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)
    (1): LayerNorm2d(
      (norm): GroupNorm(512, 512, eps=1e-05, affine=True)
    )
    (2): ReLU()
    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (4): LayerNorm2d(
      (norm): GroupNorm(256, 256, eps=1e-05, affine=True)
    )
    (5): ReLU()
    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (7): LayerNorm2d(
      (norm): GroupNorm(128, 128, eps=1e-05, affine=True)
    )
    (8): ReLU()
    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (10): LayerNorm2d(
      (norm): GroupNorm(64, 64, eps=1e-05, affine=True)
    )
    (11): ReLU()
    (12): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (13): LayerNorm2d(
      (norm): GroupNorm(32, 32, eps=1e-05, affine=True)
    )
    (14): ReLU()
    (15): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  )
)
Discriminator(
  (net): Sequential(
    (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2)
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (3): LayerNorm2d(
      (norm): GroupNorm(64, 64, eps=1e-05, affine=True)
    )
    (4): LeakyReLU(negative_slope=0.2)
    (5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (6): LayerNorm2d(
      (norm): GroupNorm(128, 128, eps=1e-05, affine=True)
    )
    (7): LeakyReLU(negative_slope=0.2)
    (8): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (9): LayerNorm2d(
      (norm): GroupNorm(256, 256, eps=1e-05, affine=True)
    )
    (10): LeakyReLU(negative_slope=0.2)
    (11): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (12): LayerNorm2d(
      (norm): GroupNorm(512, 512, eps=1e-05, affine=True)
    )
    (13): LeakyReLU(negative_slope=0.2)
    (14): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)
  )
)

Epoch 1/10
Iteration 1, D: 3918.0400390625, GP: 3918.5927734375, Gradient norm: 20.748546600341797
Traceback (most recent call last):
  File "C:\Users\dell\Desktop\git\main.py", line 103, in <module>
    main(config)
  File "C:\Users\dell\Desktop\git\main.py", line 72, in main
    trainer.train(data_loader, epochs, save_training_gif=False)
  File "C:\Users\dell\Desktop\git\training.py", line 174, in train
    self._train_epoch(data_loader)
  File "C:\Users\dell\Desktop\git\training.py", line 127, in _train_epoch
    self._critic_train_iteration(data)
  File "C:\Users\dell\Desktop\git\training.py", line 53, in _critic_train_iteration
    d_loss.backward()
  File "C:\Users\dell\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "C:\Users\dell\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\autograd\__init__.py", line 267, in backward
    _engine_run_backward(
  File "C:\Users\dell\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\autograd\graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
